{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmAWPXyKcATrylihCRsCbO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelaaulina/UAS_G.231.20.0164/blob/main/UASKasusA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ofaIV_ccbCRd",
        "outputId": "7bfe3cf2-cba2-4838-bcc9-dc75b6671c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
              "0      1            5.1           3.5            1.4           0.2   \n",
              "1      2            4.9           3.0            1.4           0.2   \n",
              "2      3            4.7           3.2            1.3           0.2   \n",
              "3      4            4.6           3.1            1.5           0.2   \n",
              "4      5            5.0           3.6            1.4           0.2   \n",
              "..   ...            ...           ...            ...           ...   \n",
              "145  146            6.7           3.0            5.2           2.3   \n",
              "146  147            6.3           2.5            5.0           1.9   \n",
              "147  148            6.5           3.0            5.2           2.0   \n",
              "148  149            6.2           3.4            5.4           2.3   \n",
              "149  150            5.9           3.0            5.1           1.8   \n",
              "\n",
              "            Species  \n",
              "0       Iris-setosa  \n",
              "1       Iris-setosa  \n",
              "2       Iris-setosa  \n",
              "3       Iris-setosa  \n",
              "4       Iris-setosa  \n",
              "..              ...  \n",
              "145  Iris-virginica  \n",
              "146  Iris-virginica  \n",
              "147  Iris-virginica  \n",
              "148  Iris-virginica  \n",
              "149  Iris-virginica  \n",
              "\n",
              "[150 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00c3edd7-6945-4ac2-b1ff-06254b3953cc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>146</td>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>147</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>148</td>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>149</td>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>150</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00c3edd7-6945-4ac2-b1ff-06254b3953cc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00c3edd7-6945-4ac2-b1ff-06254b3953cc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00c3edd7-6945-4ac2-b1ff-06254b3953cc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load irisAll.csv files as a Pandas DataFrame\n",
        "# https://www.kaggle.com/arshid/iris-flower-dataset\n",
        "data = pd.read_csv(\"Iris.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Species'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy3rcvnQbSV-",
        "outputId": "ed8052a9-8d3a-4cad-b65c-9ae779afe9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some information about dataset\n",
        "print (data.shape)\n",
        "print(type(data)) # 'pandas.core.frame.DataFrame'\n",
        "print(data.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivupDwRubZvk",
        "outputId": "a7635526-354a-4ee1-949e-39e970c25ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 6)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Id                 int64\n",
            "SepalLengthCm    float64\n",
            "SepalWidthCm     float64\n",
            "PetalLengthCm    float64\n",
            "PetalWidthCm     float64\n",
            "Species           object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pT9xIoYbbkwr",
        "outputId": "82de7e53-9f91-4c3f-da99-d7c13c2e4750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
              "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f164c93-b958-4208-8103-3fc773ce72a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f164c93-b958-4208-8103-3fc773ce72a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f164c93-b958-4208-8103-3fc773ce72a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f164c93-b958-4208-8103-3fc773ce72a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "gpbkeDpPbnjY",
        "outputId": "2d78d242-5324-488b-d114-c8533260adbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
              "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
              "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
              "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
              "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
              "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
              "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
              "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
              "max    150.000000       7.900000      4.400000       6.900000      2.500000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-015fba45-4828-4520-88d9-9280a989f61c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>75.500000</td>\n",
              "      <td>5.843333</td>\n",
              "      <td>3.054000</td>\n",
              "      <td>3.758667</td>\n",
              "      <td>1.198667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>43.445368</td>\n",
              "      <td>0.828066</td>\n",
              "      <td>0.433594</td>\n",
              "      <td>1.764420</td>\n",
              "      <td>0.763161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.300000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>38.250000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>75.500000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>1.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>112.750000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>1.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>150.000000</td>\n",
              "      <td>7.900000</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>6.900000</td>\n",
              "      <td>2.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-015fba45-4828-4520-88d9-9280a989f61c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-015fba45-4828-4520-88d9-9280a989f61c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-015fba45-4828-4520-88d9-9280a989f61c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input and output data\n",
        "# for training models\n",
        "y = data.pop('Species')\n",
        "data.drop('Id', inplace=True, axis=1)\n",
        "#  inplace=True means the operation would work on the original object.\n",
        "#  axis=1 means we are dropping the column, not the row.\n",
        "X=data\n",
        "\n",
        "print(X.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eQtx8bvbrhQ",
        "outputId": "2a87c403-e8b6-4510-b2ec-aab81853af87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
            "0            5.1           3.5            1.4           0.2\n",
            "1            4.9           3.0            1.4           0.2\n",
            "2            4.7           3.2            1.3           0.2\n",
            "3            4.6           3.1            1.5           0.2\n",
            "4            5.0           3.6            1.4           0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TEdBLrwbuAx",
        "outputId": "2a08e375-a191-4546-8fe2-b62ac68f1ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    Iris-setosa\n",
            "1    Iris-setosa\n",
            "2    Iris-setosa\n",
            "3    Iris-setosa\n",
            "4    Iris-setosa\n",
            "Name: Species, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, test_size=0.2)\n",
        "sc_X = StandardScaler()\n",
        "X_trainscaled=sc_X.fit_transform(X_train)\n",
        "X_testscaled=sc_X.transform(X_test)"
      ],
      "metadata": {
        "id": "zj7MbjC_bxlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ('identity', 'logistic', 'tanh', 'relu')\n",
        "net = MLPClassifier(hidden_layer_sizes=(64,32),activation=\"relu\",\n",
        "                    random_state=1,alpha=1) # (256,128,64,32)\n",
        "\n",
        "# Hidden layer activation functions {â€˜identityâ€™, â€˜logisticâ€™, â€˜tanhâ€™, â€˜reluâ€™}, default=â€™reluâ€™\n",
        "# Activation function for the all hidden layers is the same\n",
        "# solver{â€˜lbfgsâ€™, â€˜sgdâ€™, â€˜adamâ€™}, default=â€™adamâ€™\n",
        "# MLPClassifier uses the Average Cross-Entropy loss function. In the current version you can not set this parameter.\n",
        "# MLPClassifier uses softmax transfer function as output activation function for Multiclass-Classification. In the current version you can not set this parameter.\n",
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "_S0cpuRPb0eS",
        "outputId": "56d79e31-711f-4064-cc0d-712a8ea43830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=1, hidden_layer_sizes=(64, 32), random_state=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1, hidden_layer_sizes=(64, 32), random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1, hidden_layer_sizes=(64, 32), random_state=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.fit(X_trainscaled, y_train)\n",
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "_iqqa93kb3S9",
        "outputId": "ebd750f3-6f42-4634-cc21-800937deb2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=1, hidden_layer_sizes=(64, 32), random_state=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1, hidden_layer_sizes=(64, 32), random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1, hidden_layer_sizes=(64, 32), random_state=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[coef.shape for coef in net.coefs_]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG8pxRtYb6Yb",
        "outputId": "c325fc51-88a9-41c5-d7b7-234b7e38e6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 64), (64, 32), (32, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net.out_activation_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df5fTLh8b_I9",
        "outputId": "25c35fba-131e-4362-f53b-8d722f308c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=net.predict(X_testscaled)\n",
        "print(net.score(X_testscaled, y_test))\n",
        "\n",
        "print('Test Accuracy : %.3f'%net.score(X_testscaled, y_test))\n",
        "print('Training Accuracy : %.3f'%net.score(X_trainscaled, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHzbgEF6cBKw",
        "outputId": "d7e7c30f-04b3-4dff-dc14-654030a3a620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9666666666666667\n",
            "Test Accuracy : 0.967\n",
            "Training Accuracy : 0.983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(cf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXMJT-cNcDbL",
        "outputId": "d691dfe0-1b48-411e-d4e9-dd6157537c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0  6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net.predict([[6,1,2,1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5wr9NurcF5t",
        "outputId": "6466042e-cb00-444a-aaa8-c40c848b0c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Iris-virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred[:4])\n",
        "print(y_test[:4].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHVY5fc-cIvo",
        "outputId": "28d6f486-5c5c-4999-9dec-5ec286258462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Iris-setosa' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa']\n",
            "['Iris-setosa' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Activation function for hidden layers : {}'.format(net.activation))\n",
        "print('Solver : {}'.format(net.solver))\n",
        "print('Alpha value : {}'.format(net.alpha))\n",
        "# activation{â€˜identityâ€™, â€˜logisticâ€™, â€˜tanhâ€™, â€˜reluâ€™}, default=â€™reluâ€™\n",
        "# solver{â€˜lbfgsâ€™, â€˜sgdâ€™, â€˜adamâ€™}, default=â€™adamâ€™\n",
        "print('Output activation by default: {}'.format(net.out_activation_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMo9u5GacLIJ",
        "outputId": "aaae8b90-9623-4598-a6c0-9e26dc265255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation function for hidden layers : relu\n",
            "Solver : adam\n",
            "Alpha value : 1\n",
            "Output activation by default: softmax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test Accuracy : %.3f'%net.score(X_testscaled, y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%net.score(X_trainscaled, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0IcruuUcN43",
        "outputId": "de937634-4ccd-44f7-e8b9-5ab3c4731e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy : 0.967\n",
            "Training Accuracy : 0.983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as pyplot"
      ],
      "metadata": {
        "id": "fKxWe52JcQg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luyVafSkcTFL",
        "outputId": "d0567931-b668-4ff4-a357-525e5812b30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         Iris-setosa\n",
              "1         Iris-setosa\n",
              "2         Iris-setosa\n",
              "3         Iris-setosa\n",
              "4         Iris-setosa\n",
              "            ...      \n",
              "145    Iris-virginica\n",
              "146    Iris-virginica\n",
              "147    Iris-virginica\n",
              "148    Iris-virginica\n",
              "149    Iris-virginica\n",
              "Name: Species, Length: 150, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras requires your output feature to be one-hot encoded values.\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "### Categorical data to be converted to numeric data\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "y_number= label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "GvhWN04ZcW9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocZhjuFAcaKc",
        "outputId": "aa9bfd42-297e-4e3a-e53c-6d37e65aae7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = to_categorical(y_number)\n",
        "print(Y[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrIjnew3cdNn",
        "outputId": "d4d09bc1-3f06-4605-e079-b45d28528c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GnTjj8WcfjN",
        "outputId": "675f384e-d05b-4ba0-b39b-118c6a42c8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state=1, test_size=0.2)\n",
        "sc_X = StandardScaler()\n",
        "X_trainscaled=sc_X.fit_transform(X_train)\n",
        "X_testscaled=sc_X.transform(X_test)"
      ],
      "metadata": {
        "id": "K1XbxLOLciKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=(4), activation='relu'))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_trainscaled, y_train, validation_data=(X_testscaled,y_test),\n",
        "                    epochs=300, batch_size=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA92zeIbcl5b",
        "outputId": "72683fa4-437c-444e-e69a-0be95c1d7078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "18/18 [==============================] - 2s 34ms/step - loss: 1.0911 - accuracy: 0.4333 - val_loss: 1.1022 - val_accuracy: 0.2667\n",
            "Epoch 2/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 1.0786 - accuracy: 0.5167 - val_loss: 1.0958 - val_accuracy: 0.3000\n",
            "Epoch 3/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 1.0654 - accuracy: 0.4417 - val_loss: 1.0905 - val_accuracy: 0.3000\n",
            "Epoch 4/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 1.0506 - accuracy: 0.4417 - val_loss: 1.0842 - val_accuracy: 0.3000\n",
            "Epoch 5/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 1.0287 - accuracy: 0.4833 - val_loss: 1.0742 - val_accuracy: 0.4000\n",
            "Epoch 6/300\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 1.0000 - accuracy: 0.6000 - val_loss: 1.0574 - val_accuracy: 0.4000\n",
            "Epoch 7/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9674 - accuracy: 0.6250 - val_loss: 1.0389 - val_accuracy: 0.4000\n",
            "Epoch 8/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.9254 - accuracy: 0.6417 - val_loss: 1.0081 - val_accuracy: 0.5000\n",
            "Epoch 9/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.8783 - accuracy: 0.6667 - val_loss: 0.9797 - val_accuracy: 0.5000\n",
            "Epoch 10/300\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.8340 - accuracy: 0.6667 - val_loss: 0.9507 - val_accuracy: 0.5000\n",
            "Epoch 11/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.8005 - accuracy: 0.6833 - val_loss: 0.9291 - val_accuracy: 0.5667\n",
            "Epoch 12/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.7708 - accuracy: 0.7167 - val_loss: 0.9041 - val_accuracy: 0.6000\n",
            "Epoch 13/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.7418 - accuracy: 0.7667 - val_loss: 0.8801 - val_accuracy: 0.6333\n",
            "Epoch 14/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7153 - accuracy: 0.8167 - val_loss: 0.8566 - val_accuracy: 0.6333\n",
            "Epoch 15/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6961 - accuracy: 0.8083 - val_loss: 0.8426 - val_accuracy: 0.6333\n",
            "Epoch 16/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6758 - accuracy: 0.8333 - val_loss: 0.8044 - val_accuracy: 0.7333\n",
            "Epoch 17/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6551 - accuracy: 0.8583 - val_loss: 0.7994 - val_accuracy: 0.7333\n",
            "Epoch 18/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6368 - accuracy: 0.8667 - val_loss: 0.7684 - val_accuracy: 0.7667\n",
            "Epoch 19/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6201 - accuracy: 0.8583 - val_loss: 0.7654 - val_accuracy: 0.7667\n",
            "Epoch 20/300\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6046 - accuracy: 0.8917 - val_loss: 0.7308 - val_accuracy: 0.8333\n",
            "Epoch 21/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5888 - accuracy: 0.8917 - val_loss: 0.7291 - val_accuracy: 0.8000\n",
            "Epoch 22/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5717 - accuracy: 0.9083 - val_loss: 0.6971 - val_accuracy: 0.8333\n",
            "Epoch 23/300\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5584 - accuracy: 0.9167 - val_loss: 0.6823 - val_accuracy: 0.8333\n",
            "Epoch 24/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5427 - accuracy: 0.9167 - val_loss: 0.6742 - val_accuracy: 0.8333\n",
            "Epoch 25/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5354 - accuracy: 0.9167 - val_loss: 0.6709 - val_accuracy: 0.8333\n",
            "Epoch 26/300\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5151 - accuracy: 0.9250 - val_loss: 0.6190 - val_accuracy: 0.8333\n",
            "Epoch 27/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5045 - accuracy: 0.9417 - val_loss: 0.6167 - val_accuracy: 0.8333\n",
            "Epoch 28/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4933 - accuracy: 0.9417 - val_loss: 0.5937 - val_accuracy: 0.8667\n",
            "Epoch 29/300\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4820 - accuracy: 0.9667 - val_loss: 0.5785 - val_accuracy: 0.8667\n",
            "Epoch 30/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.4680 - accuracy: 0.9667 - val_loss: 0.5795 - val_accuracy: 0.8333\n",
            "Epoch 31/300\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4603 - accuracy: 0.9167 - val_loss: 0.5976 - val_accuracy: 0.8333\n",
            "Epoch 32/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4505 - accuracy: 0.9333 - val_loss: 0.5605 - val_accuracy: 0.8333\n",
            "Epoch 33/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4389 - accuracy: 0.9667 - val_loss: 0.5375 - val_accuracy: 0.8667\n",
            "Epoch 34/300\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.4298 - accuracy: 0.9667 - val_loss: 0.5241 - val_accuracy: 0.8667\n",
            "Epoch 35/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.4213 - accuracy: 0.9667 - val_loss: 0.5177 - val_accuracy: 0.8667\n",
            "Epoch 36/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.4115 - accuracy: 0.9667 - val_loss: 0.5059 - val_accuracy: 0.9000\n",
            "Epoch 37/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4034 - accuracy: 0.9667 - val_loss: 0.4903 - val_accuracy: 0.9000\n",
            "Epoch 38/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3971 - accuracy: 0.9667 - val_loss: 0.4911 - val_accuracy: 0.9000\n",
            "Epoch 39/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3864 - accuracy: 0.9667 - val_loss: 0.4653 - val_accuracy: 0.9000\n",
            "Epoch 40/300\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3805 - accuracy: 0.9667 - val_loss: 0.4500 - val_accuracy: 0.9667\n",
            "Epoch 41/300\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.3730 - accuracy: 0.9667 - val_loss: 0.4490 - val_accuracy: 0.9333\n",
            "Epoch 42/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3669 - accuracy: 0.9667 - val_loss: 0.4295 - val_accuracy: 0.9667\n",
            "Epoch 43/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3586 - accuracy: 0.9667 - val_loss: 0.4278 - val_accuracy: 0.9333\n",
            "Epoch 44/300\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.3514 - accuracy: 0.9667 - val_loss: 0.4216 - val_accuracy: 0.9333\n",
            "Epoch 45/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3447 - accuracy: 0.9667 - val_loss: 0.4085 - val_accuracy: 0.9667\n",
            "Epoch 46/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3405 - accuracy: 0.9667 - val_loss: 0.3930 - val_accuracy: 0.9667\n",
            "Epoch 47/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3332 - accuracy: 0.9667 - val_loss: 0.3886 - val_accuracy: 0.9667\n",
            "Epoch 48/300\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 0.3260 - accuracy: 0.9667 - val_loss: 0.3722 - val_accuracy: 0.9667\n",
            "Epoch 49/300\n",
            "18/18 [==============================] - 0s 14ms/step - loss: 0.3199 - accuracy: 0.9667 - val_loss: 0.3722 - val_accuracy: 0.9667\n",
            "Epoch 50/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3150 - accuracy: 0.9667 - val_loss: 0.3540 - val_accuracy: 0.9667\n",
            "Epoch 51/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3132 - accuracy: 0.9667 - val_loss: 0.3525 - val_accuracy: 0.9667\n",
            "Epoch 52/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3022 - accuracy: 0.9667 - val_loss: 0.3422 - val_accuracy: 0.9667\n",
            "Epoch 53/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3023 - accuracy: 0.9583 - val_loss: 0.3337 - val_accuracy: 0.9667\n",
            "Epoch 54/300\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2946 - accuracy: 0.9667 - val_loss: 0.3400 - val_accuracy: 0.9667\n",
            "Epoch 55/300\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2907 - accuracy: 0.9667 - val_loss: 0.3185 - val_accuracy: 0.9667\n",
            "Epoch 56/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2818 - accuracy: 0.9667 - val_loss: 0.3187 - val_accuracy: 0.9667\n",
            "Epoch 57/300\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2790 - accuracy: 0.9667 - val_loss: 0.3105 - val_accuracy: 0.9667\n",
            "Epoch 58/300\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2730 - accuracy: 0.9667 - val_loss: 0.3040 - val_accuracy: 0.9667\n",
            "Epoch 59/300\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.2695 - accuracy: 0.9667 - val_loss: 0.2946 - val_accuracy: 0.9667\n",
            "Epoch 60/300\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2634 - accuracy: 0.9667 - val_loss: 0.2962 - val_accuracy: 0.9667\n",
            "Epoch 61/300\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 0.2612 - accuracy: 0.9667 - val_loss: 0.2888 - val_accuracy: 0.9667\n",
            "Epoch 62/300\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.2571 - accuracy: 0.9750 - val_loss: 0.2802 - val_accuracy: 0.9667\n",
            "Epoch 63/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2518 - accuracy: 0.9667 - val_loss: 0.2739 - val_accuracy: 0.9667\n",
            "Epoch 64/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2483 - accuracy: 0.9667 - val_loss: 0.2618 - val_accuracy: 0.9667\n",
            "Epoch 65/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2439 - accuracy: 0.9583 - val_loss: 0.2684 - val_accuracy: 0.9667\n",
            "Epoch 66/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2403 - accuracy: 0.9750 - val_loss: 0.2661 - val_accuracy: 0.9667\n",
            "Epoch 67/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2378 - accuracy: 0.9667 - val_loss: 0.2517 - val_accuracy: 0.9667\n",
            "Epoch 68/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2397 - accuracy: 0.9667 - val_loss: 0.2587 - val_accuracy: 0.9667\n",
            "Epoch 69/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2316 - accuracy: 0.9667 - val_loss: 0.2428 - val_accuracy: 0.9667\n",
            "Epoch 70/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2292 - accuracy: 0.9667 - val_loss: 0.2573 - val_accuracy: 0.9667\n",
            "Epoch 71/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2238 - accuracy: 0.9750 - val_loss: 0.2418 - val_accuracy: 0.9667\n",
            "Epoch 72/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2203 - accuracy: 0.9750 - val_loss: 0.2359 - val_accuracy: 0.9667\n",
            "Epoch 73/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2169 - accuracy: 0.9750 - val_loss: 0.2286 - val_accuracy: 0.9667\n",
            "Epoch 74/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2133 - accuracy: 0.9833 - val_loss: 0.2226 - val_accuracy: 0.9667\n",
            "Epoch 75/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9917 - val_loss: 0.2229 - val_accuracy: 0.9667\n",
            "Epoch 76/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2079 - accuracy: 0.9833 - val_loss: 0.2203 - val_accuracy: 0.9667\n",
            "Epoch 77/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2060 - accuracy: 0.9917 - val_loss: 0.2161 - val_accuracy: 0.9667\n",
            "Epoch 78/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2047 - accuracy: 0.9833 - val_loss: 0.2169 - val_accuracy: 0.9667\n",
            "Epoch 79/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1994 - accuracy: 0.9917 - val_loss: 0.2101 - val_accuracy: 0.9667\n",
            "Epoch 80/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1977 - accuracy: 0.9833 - val_loss: 0.2147 - val_accuracy: 0.9667\n",
            "Epoch 81/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1963 - accuracy: 0.9917 - val_loss: 0.2097 - val_accuracy: 0.9667\n",
            "Epoch 82/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1928 - accuracy: 0.9917 - val_loss: 0.2020 - val_accuracy: 0.9667\n",
            "Epoch 83/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1906 - accuracy: 0.9917 - val_loss: 0.1977 - val_accuracy: 0.9667\n",
            "Epoch 84/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1885 - accuracy: 0.9833 - val_loss: 0.1982 - val_accuracy: 0.9667\n",
            "Epoch 85/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1856 - accuracy: 0.9917 - val_loss: 0.1985 - val_accuracy: 0.9667\n",
            "Epoch 86/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1838 - accuracy: 0.9833 - val_loss: 0.1882 - val_accuracy: 0.9667\n",
            "Epoch 87/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1806 - accuracy: 0.9917 - val_loss: 0.1887 - val_accuracy: 0.9667\n",
            "Epoch 88/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1826 - accuracy: 0.9917 - val_loss: 0.1867 - val_accuracy: 0.9667\n",
            "Epoch 89/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9917 - val_loss: 0.1746 - val_accuracy: 0.9667\n",
            "Epoch 90/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1764 - accuracy: 0.9833 - val_loss: 0.1805 - val_accuracy: 0.9667\n",
            "Epoch 91/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1735 - accuracy: 0.9917 - val_loss: 0.1837 - val_accuracy: 0.9667\n",
            "Epoch 92/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1772 - accuracy: 0.9833 - val_loss: 0.1732 - val_accuracy: 0.9667\n",
            "Epoch 93/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1715 - accuracy: 0.9917 - val_loss: 0.1850 - val_accuracy: 0.9667\n",
            "Epoch 94/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9833 - val_loss: 0.1677 - val_accuracy: 0.9667\n",
            "Epoch 95/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1652 - accuracy: 0.9833 - val_loss: 0.1695 - val_accuracy: 0.9667\n",
            "Epoch 96/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1639 - accuracy: 0.9917 - val_loss: 0.1735 - val_accuracy: 0.9667\n",
            "Epoch 97/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1719 - accuracy: 0.9833 - val_loss: 0.1847 - val_accuracy: 0.9667\n",
            "Epoch 98/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9750 - val_loss: 0.1579 - val_accuracy: 0.9667\n",
            "Epoch 99/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9750 - val_loss: 0.1595 - val_accuracy: 0.9667\n",
            "Epoch 100/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9833 - val_loss: 0.1656 - val_accuracy: 0.9667\n",
            "Epoch 101/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1532 - accuracy: 0.9917 - val_loss: 0.1517 - val_accuracy: 0.9667\n",
            "Epoch 102/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9833 - val_loss: 0.1505 - val_accuracy: 0.9667\n",
            "Epoch 103/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9917 - val_loss: 0.1553 - val_accuracy: 0.9667\n",
            "Epoch 104/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1487 - accuracy: 0.9917 - val_loss: 0.1562 - val_accuracy: 0.9667\n",
            "Epoch 105/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1506 - accuracy: 0.9833 - val_loss: 0.1547 - val_accuracy: 0.9667\n",
            "Epoch 106/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9917 - val_loss: 0.1574 - val_accuracy: 0.9667\n",
            "Epoch 107/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9917 - val_loss: 0.1461 - val_accuracy: 0.9667\n",
            "Epoch 108/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9917 - val_loss: 0.1442 - val_accuracy: 0.9667\n",
            "Epoch 109/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1419 - accuracy: 0.9833 - val_loss: 0.1405 - val_accuracy: 0.9667\n",
            "Epoch 110/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9917 - val_loss: 0.1448 - val_accuracy: 0.9667\n",
            "Epoch 111/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9917 - val_loss: 0.1420 - val_accuracy: 0.9667\n",
            "Epoch 112/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9917 - val_loss: 0.1424 - val_accuracy: 0.9667\n",
            "Epoch 113/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9833 - val_loss: 0.1331 - val_accuracy: 0.9667\n",
            "Epoch 114/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9833 - val_loss: 0.1366 - val_accuracy: 0.9667\n",
            "Epoch 115/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.9833 - val_loss: 0.1298 - val_accuracy: 0.9667\n",
            "Epoch 116/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.9833 - val_loss: 0.1387 - val_accuracy: 0.9667\n",
            "Epoch 117/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1304 - accuracy: 0.9917 - val_loss: 0.1366 - val_accuracy: 0.9667\n",
            "Epoch 118/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.9917 - val_loss: 0.1348 - val_accuracy: 0.9667\n",
            "Epoch 119/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1311 - accuracy: 0.9917 - val_loss: 0.1385 - val_accuracy: 0.9667\n",
            "Epoch 120/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1294 - accuracy: 0.9833 - val_loss: 0.1236 - val_accuracy: 0.9667\n",
            "Epoch 121/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1269 - accuracy: 0.9917 - val_loss: 0.1469 - val_accuracy: 0.9667\n",
            "Epoch 122/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9917 - val_loss: 0.1360 - val_accuracy: 0.9667\n",
            "Epoch 123/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9833 - val_loss: 0.1306 - val_accuracy: 0.9667\n",
            "Epoch 124/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1204 - accuracy: 0.9917 - val_loss: 0.1297 - val_accuracy: 0.9667\n",
            "Epoch 125/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9917 - val_loss: 0.1270 - val_accuracy: 0.9667\n",
            "Epoch 126/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9917 - val_loss: 0.1342 - val_accuracy: 0.9667\n",
            "Epoch 127/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1200 - accuracy: 0.9917 - val_loss: 0.1301 - val_accuracy: 0.9667\n",
            "Epoch 128/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1171 - accuracy: 0.9917 - val_loss: 0.1243 - val_accuracy: 0.9667\n",
            "Epoch 129/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9833 - val_loss: 0.1234 - val_accuracy: 0.9667\n",
            "Epoch 130/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9917 - val_loss: 0.1241 - val_accuracy: 0.9667\n",
            "Epoch 131/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9917 - val_loss: 0.1178 - val_accuracy: 0.9667\n",
            "Epoch 132/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.9917 - val_loss: 0.1198 - val_accuracy: 0.9667\n",
            "Epoch 133/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9833 - val_loss: 0.1140 - val_accuracy: 0.9667\n",
            "Epoch 134/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9917 - val_loss: 0.1192 - val_accuracy: 0.9667\n",
            "Epoch 135/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9917 - val_loss: 0.1192 - val_accuracy: 0.9667\n",
            "Epoch 136/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9750 - val_loss: 0.1075 - val_accuracy: 0.9667\n",
            "Epoch 137/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9917 - val_loss: 0.1188 - val_accuracy: 0.9667\n",
            "Epoch 138/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9833 - val_loss: 0.1128 - val_accuracy: 0.9667\n",
            "Epoch 139/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9917 - val_loss: 0.1074 - val_accuracy: 0.9667\n",
            "Epoch 140/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9917 - val_loss: 0.1129 - val_accuracy: 0.9667\n",
            "Epoch 141/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.9750 - val_loss: 0.1038 - val_accuracy: 0.9667\n",
            "Epoch 142/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1063 - accuracy: 0.9917 - val_loss: 0.1187 - val_accuracy: 0.9667\n",
            "Epoch 143/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.9750 - val_loss: 0.1006 - val_accuracy: 0.9667\n",
            "Epoch 144/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9917 - val_loss: 0.1037 - val_accuracy: 0.9667\n",
            "Epoch 145/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1027 - accuracy: 0.9917 - val_loss: 0.1066 - val_accuracy: 0.9667\n",
            "Epoch 146/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.9917 - val_loss: 0.1095 - val_accuracy: 0.9667\n",
            "Epoch 147/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9917 - val_loss: 0.1045 - val_accuracy: 0.9667\n",
            "Epoch 148/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9833 - val_loss: 0.0972 - val_accuracy: 0.9667\n",
            "Epoch 149/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9833 - val_loss: 0.1035 - val_accuracy: 0.9667\n",
            "Epoch 150/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9917 - val_loss: 0.1074 - val_accuracy: 0.9667\n",
            "Epoch 151/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9917 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
            "Epoch 152/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9917 - val_loss: 0.1022 - val_accuracy: 0.9667\n",
            "Epoch 153/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0944 - accuracy: 0.9917 - val_loss: 0.1009 - val_accuracy: 0.9667\n",
            "Epoch 154/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9917 - val_loss: 0.1001 - val_accuracy: 0.9667\n",
            "Epoch 155/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1031 - accuracy: 0.9750 - val_loss: 0.0878 - val_accuracy: 0.9667\n",
            "Epoch 156/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.9833 - val_loss: 0.1042 - val_accuracy: 0.9667\n",
            "Epoch 157/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9917 - val_loss: 0.1014 - val_accuracy: 0.9667\n",
            "Epoch 158/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9917 - val_loss: 0.0945 - val_accuracy: 0.9667\n",
            "Epoch 159/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9833 - val_loss: 0.0964 - val_accuracy: 0.9667\n",
            "Epoch 160/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9917 - val_loss: 0.0986 - val_accuracy: 0.9667\n",
            "Epoch 161/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9833 - val_loss: 0.0935 - val_accuracy: 0.9667\n",
            "Epoch 162/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9833 - val_loss: 0.0965 - val_accuracy: 0.9667\n",
            "Epoch 163/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9917 - val_loss: 0.0979 - val_accuracy: 0.9667\n",
            "Epoch 164/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9917 - val_loss: 0.0989 - val_accuracy: 0.9667\n",
            "Epoch 165/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9917 - val_loss: 0.0898 - val_accuracy: 0.9667\n",
            "Epoch 166/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9750 - val_loss: 0.0883 - val_accuracy: 0.9667\n",
            "Epoch 167/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0851 - accuracy: 0.9917 - val_loss: 0.0968 - val_accuracy: 0.9667\n",
            "Epoch 168/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.9917 - val_loss: 0.0901 - val_accuracy: 0.9667\n",
            "Epoch 169/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9833 - val_loss: 0.0881 - val_accuracy: 0.9667\n",
            "Epoch 170/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9917 - val_loss: 0.0923 - val_accuracy: 0.9667\n",
            "Epoch 171/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9917 - val_loss: 0.0969 - val_accuracy: 0.9667\n",
            "Epoch 172/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0874 - accuracy: 0.9833 - val_loss: 0.0810 - val_accuracy: 0.9667\n",
            "Epoch 173/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9917 - val_loss: 0.0924 - val_accuracy: 0.9667\n",
            "Epoch 174/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9833 - val_loss: 0.1259 - val_accuracy: 0.9667\n",
            "Epoch 175/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9917 - val_loss: 0.0928 - val_accuracy: 0.9667\n",
            "Epoch 176/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9833 - val_loss: 0.0827 - val_accuracy: 0.9667\n",
            "Epoch 177/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9750 - val_loss: 0.0883 - val_accuracy: 0.9667\n",
            "Epoch 178/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9917 - val_loss: 0.0912 - val_accuracy: 0.9667\n",
            "Epoch 179/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9833 - val_loss: 0.1208 - val_accuracy: 0.9667\n",
            "Epoch 180/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9833 - val_loss: 0.0911 - val_accuracy: 0.9667\n",
            "Epoch 181/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0789 - accuracy: 0.9917 - val_loss: 0.0752 - val_accuracy: 0.9667\n",
            "Epoch 182/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.9833 - val_loss: 0.0771 - val_accuracy: 0.9667\n",
            "Epoch 183/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9917 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
            "Epoch 184/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9917 - val_loss: 0.0843 - val_accuracy: 0.9667\n",
            "Epoch 185/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9750 - val_loss: 0.0755 - val_accuracy: 0.9667\n",
            "Epoch 186/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.9917 - val_loss: 0.0801 - val_accuracy: 0.9667\n",
            "Epoch 187/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9917 - val_loss: 0.0808 - val_accuracy: 0.9667\n",
            "Epoch 188/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9917 - val_loss: 0.0803 - val_accuracy: 0.9667\n",
            "Epoch 189/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.9833 - val_loss: 0.0783 - val_accuracy: 0.9667\n",
            "Epoch 190/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9917 - val_loss: 0.0846 - val_accuracy: 0.9667\n",
            "Epoch 191/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0734 - accuracy: 0.9917 - val_loss: 0.0789 - val_accuracy: 0.9667\n",
            "Epoch 192/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9917 - val_loss: 0.0790 - val_accuracy: 0.9667\n",
            "Epoch 193/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9917 - val_loss: 0.0800 - val_accuracy: 0.9667\n",
            "Epoch 194/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0724 - accuracy: 0.9917 - val_loss: 0.0769 - val_accuracy: 0.9667\n",
            "Epoch 195/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9917 - val_loss: 0.0763 - val_accuracy: 0.9667\n",
            "Epoch 196/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9917 - val_loss: 0.0766 - val_accuracy: 0.9667\n",
            "Epoch 197/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9917 - val_loss: 0.0731 - val_accuracy: 0.9667\n",
            "Epoch 198/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9917 - val_loss: 0.0733 - val_accuracy: 0.9667\n",
            "Epoch 199/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9833 - val_loss: 0.0736 - val_accuracy: 0.9667\n",
            "Epoch 200/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0710 - accuracy: 0.9917 - val_loss: 0.0848 - val_accuracy: 0.9667\n",
            "Epoch 201/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.9917 - val_loss: 0.0747 - val_accuracy: 0.9667\n",
            "Epoch 202/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9917 - val_loss: 0.0727 - val_accuracy: 0.9667\n",
            "Epoch 203/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.9917 - val_loss: 0.0748 - val_accuracy: 0.9667\n",
            "Epoch 204/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.9833 - val_loss: 0.0719 - val_accuracy: 0.9667\n",
            "Epoch 205/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9917 - val_loss: 0.0754 - val_accuracy: 0.9667\n",
            "Epoch 206/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0676 - accuracy: 0.9917 - val_loss: 0.0776 - val_accuracy: 0.9667\n",
            "Epoch 207/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9917 - val_loss: 0.1075 - val_accuracy: 0.9667\n",
            "Epoch 208/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9917 - val_loss: 0.0806 - val_accuracy: 0.9667\n",
            "Epoch 209/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9917 - val_loss: 0.0679 - val_accuracy: 0.9667\n",
            "Epoch 210/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9750 - val_loss: 0.0624 - val_accuracy: 0.9667\n",
            "Epoch 211/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0652 - accuracy: 0.9917 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
            "Epoch 212/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9917 - val_loss: 0.0797 - val_accuracy: 0.9667\n",
            "Epoch 213/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0642 - accuracy: 0.9917 - val_loss: 0.0600 - val_accuracy: 0.9667\n",
            "Epoch 214/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9833 - val_loss: 0.0630 - val_accuracy: 0.9667\n",
            "Epoch 215/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0663 - accuracy: 0.9833 - val_loss: 0.0639 - val_accuracy: 0.9667\n",
            "Epoch 216/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0628 - accuracy: 0.9917 - val_loss: 0.0729 - val_accuracy: 0.9667\n",
            "Epoch 217/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0661 - accuracy: 0.9917 - val_loss: 0.0751 - val_accuracy: 0.9667\n",
            "Epoch 218/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9917 - val_loss: 0.0727 - val_accuracy: 0.9667\n",
            "Epoch 219/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 0.0664 - val_accuracy: 0.9667\n",
            "Epoch 220/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.9917 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
            "Epoch 221/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0675 - val_accuracy: 0.9667\n",
            "Epoch 222/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.9917 - val_loss: 0.0735 - val_accuracy: 0.9667\n",
            "Epoch 223/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0623 - accuracy: 0.9917 - val_loss: 0.0690 - val_accuracy: 0.9667\n",
            "Epoch 224/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9917 - val_loss: 0.0715 - val_accuracy: 0.9667\n",
            "Epoch 225/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0618 - accuracy: 0.9917 - val_loss: 0.0647 - val_accuracy: 0.9667\n",
            "Epoch 226/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0650 - accuracy: 0.9917 - val_loss: 0.0715 - val_accuracy: 0.9667\n",
            "Epoch 227/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0608 - accuracy: 0.9917 - val_loss: 0.0637 - val_accuracy: 0.9667\n",
            "Epoch 228/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0677 - val_accuracy: 0.9667\n",
            "Epoch 229/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0601 - accuracy: 0.9917 - val_loss: 0.0659 - val_accuracy: 0.9667\n",
            "Epoch 230/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0597 - accuracy: 0.9917 - val_loss: 0.0669 - val_accuracy: 0.9667\n",
            "Epoch 231/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.0656 - val_accuracy: 0.9667\n",
            "Epoch 232/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0603 - accuracy: 0.9917 - val_loss: 0.0690 - val_accuracy: 0.9667\n",
            "Epoch 233/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0599 - accuracy: 0.9917 - val_loss: 0.0728 - val_accuracy: 0.9667\n",
            "Epoch 234/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.9917 - val_loss: 0.0693 - val_accuracy: 0.9667\n",
            "Epoch 235/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9917 - val_loss: 0.0655 - val_accuracy: 0.9667\n",
            "Epoch 236/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9917 - val_loss: 0.0706 - val_accuracy: 0.9667\n",
            "Epoch 237/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9917 - val_loss: 0.0655 - val_accuracy: 0.9667\n",
            "Epoch 238/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.9917 - val_loss: 0.0670 - val_accuracy: 0.9667\n",
            "Epoch 239/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0691 - val_accuracy: 0.9667\n",
            "Epoch 240/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9917 - val_loss: 0.0703 - val_accuracy: 0.9667\n",
            "Epoch 241/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9750 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
            "Epoch 242/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0738 - val_accuracy: 0.9667\n",
            "Epoch 243/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0645 - val_accuracy: 0.9667\n",
            "Epoch 244/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9917 - val_loss: 0.0674 - val_accuracy: 0.9667\n",
            "Epoch 245/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9917 - val_loss: 0.0796 - val_accuracy: 0.9667\n",
            "Epoch 246/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0558 - accuracy: 0.9917 - val_loss: 0.0640 - val_accuracy: 0.9667\n",
            "Epoch 247/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9750 - val_loss: 0.0610 - val_accuracy: 0.9667\n",
            "Epoch 248/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9917 - val_loss: 0.0646 - val_accuracy: 0.9667\n",
            "Epoch 249/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0563 - accuracy: 0.9917 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
            "Epoch 250/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
            "Epoch 251/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.9917 - val_loss: 0.0664 - val_accuracy: 0.9667\n",
            "Epoch 252/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0650 - val_accuracy: 0.9667\n",
            "Epoch 253/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0620 - val_accuracy: 0.9667\n",
            "Epoch 254/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9917 - val_loss: 0.0756 - val_accuracy: 0.9667\n",
            "Epoch 255/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9917 - val_loss: 0.0740 - val_accuracy: 0.9667\n",
            "Epoch 256/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9917 - val_loss: 0.0615 - val_accuracy: 0.9667\n",
            "Epoch 257/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0568 - val_accuracy: 0.9667\n",
            "Epoch 258/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9917 - val_loss: 0.0643 - val_accuracy: 0.9667\n",
            "Epoch 259/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9917 - val_loss: 0.0643 - val_accuracy: 0.9667\n",
            "Epoch 260/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0543 - accuracy: 0.9917 - val_loss: 0.0628 - val_accuracy: 0.9667\n",
            "Epoch 261/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0542 - accuracy: 0.9917 - val_loss: 0.0636 - val_accuracy: 0.9667\n",
            "Epoch 262/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0607 - val_accuracy: 0.9667\n",
            "Epoch 263/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0607 - val_accuracy: 0.9667\n",
            "Epoch 264/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9917 - val_loss: 0.0716 - val_accuracy: 0.9667\n",
            "Epoch 265/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 0.9917 - val_loss: 0.0610 - val_accuracy: 0.9667\n",
            "Epoch 266/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9917 - val_loss: 0.0596 - val_accuracy: 0.9667\n",
            "Epoch 267/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9833 - val_loss: 0.0596 - val_accuracy: 0.9667\n",
            "Epoch 268/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9833 - val_loss: 0.0661 - val_accuracy: 0.9667\n",
            "Epoch 269/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9917 - val_loss: 0.0661 - val_accuracy: 0.9667\n",
            "Epoch 270/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0638 - val_accuracy: 0.9667\n",
            "Epoch 271/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9917 - val_loss: 0.0635 - val_accuracy: 0.9667\n",
            "Epoch 272/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0513 - accuracy: 0.9917 - val_loss: 0.0645 - val_accuracy: 0.9667\n",
            "Epoch 273/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9917 - val_loss: 0.0705 - val_accuracy: 0.9667\n",
            "Epoch 274/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9917 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
            "Epoch 275/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.9833 - val_loss: 0.0553 - val_accuracy: 0.9667\n",
            "Epoch 276/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9917 - val_loss: 0.0641 - val_accuracy: 0.9667\n",
            "Epoch 277/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0514 - accuracy: 0.9917 - val_loss: 0.0566 - val_accuracy: 0.9667\n",
            "Epoch 278/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9917 - val_loss: 0.0615 - val_accuracy: 0.9667\n",
            "Epoch 279/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9833 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
            "Epoch 280/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9917 - val_loss: 0.0701 - val_accuracy: 0.9667\n",
            "Epoch 281/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9917 - val_loss: 0.0592 - val_accuracy: 0.9667\n",
            "Epoch 282/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0507 - accuracy: 0.9917 - val_loss: 0.0673 - val_accuracy: 0.9667\n",
            "Epoch 283/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
            "Epoch 284/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.9917 - val_loss: 0.0646 - val_accuracy: 0.9667\n",
            "Epoch 285/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9917 - val_loss: 0.0726 - val_accuracy: 0.9667\n",
            "Epoch 286/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 0.9917 - val_loss: 0.0628 - val_accuracy: 0.9667\n",
            "Epoch 287/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0548 - val_accuracy: 0.9667\n",
            "Epoch 288/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9833 - val_loss: 0.0703 - val_accuracy: 0.9667\n",
            "Epoch 289/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9917 - val_loss: 0.0650 - val_accuracy: 0.9667\n",
            "Epoch 290/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0507 - accuracy: 0.9833 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
            "Epoch 291/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9833 - val_loss: 0.0720 - val_accuracy: 0.9667\n",
            "Epoch 292/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9917 - val_loss: 0.0687 - val_accuracy: 0.9667\n",
            "Epoch 293/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9917 - val_loss: 0.0553 - val_accuracy: 0.9667\n",
            "Epoch 294/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9833 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
            "Epoch 295/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9917 - val_loss: 0.0717 - val_accuracy: 0.9667\n",
            "Epoch 296/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.9917 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
            "Epoch 297/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 0.9750 - val_loss: 0.0562 - val_accuracy: 0.9667\n",
            "Epoch 298/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9917 - val_loss: 0.1019 - val_accuracy: 0.9667\n",
            "Epoch 299/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9917 - val_loss: 0.0627 - val_accuracy: 0.9667\n",
            "Epoch 300/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0476 - accuracy: 0.9917 - val_loss: 0.0515 - val_accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_testscaled,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4IltCMEcocj",
        "outputId": "ea8c26df-53f8-41b2-93b9-9f06a91a65cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0515 - accuracy: 0.9667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05149887874722481, 0.9666666388511658]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=(4), activation='relu',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(7, activation='relu',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "model.add(Dense(5, activation='relu', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_trainscaled, y_train, validation_data=(X_testscaled,y_test),\n",
        "                    epochs=300, batch_size=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9R3EkrPc1n5",
        "outputId": "ffd2e2a2-f965-4db4-d872-8808d153e0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "18/18 [==============================] - 3s 28ms/step - loss: 1.4269 - accuracy: 0.2667 - val_loss: 1.4394 - val_accuracy: 0.3000\n",
            "Epoch 2/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.3774 - accuracy: 0.5000 - val_loss: 1.4095 - val_accuracy: 0.3667\n",
            "Epoch 3/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.3502 - accuracy: 0.3833 - val_loss: 1.3833 - val_accuracy: 0.2000\n",
            "Epoch 4/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.3308 - accuracy: 0.5083 - val_loss: 1.3627 - val_accuracy: 0.4667\n",
            "Epoch 5/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.3123 - accuracy: 0.5833 - val_loss: 1.3429 - val_accuracy: 0.4667\n",
            "Epoch 6/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.2912 - accuracy: 0.6500 - val_loss: 1.3213 - val_accuracy: 0.5000\n",
            "Epoch 7/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.2653 - accuracy: 0.6750 - val_loss: 1.2979 - val_accuracy: 0.5333\n",
            "Epoch 8/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.2369 - accuracy: 0.6667 - val_loss: 1.2733 - val_accuracy: 0.5333\n",
            "Epoch 9/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.2071 - accuracy: 0.6833 - val_loss: 1.2467 - val_accuracy: 0.5333\n",
            "Epoch 10/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1756 - accuracy: 0.7000 - val_loss: 1.2207 - val_accuracy: 0.5333\n",
            "Epoch 11/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.1459 - accuracy: 0.7000 - val_loss: 1.1950 - val_accuracy: 0.5333\n",
            "Epoch 12/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1171 - accuracy: 0.6917 - val_loss: 1.1756 - val_accuracy: 0.5667\n",
            "Epoch 13/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0930 - accuracy: 0.7000 - val_loss: 1.1543 - val_accuracy: 0.5667\n",
            "Epoch 14/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0715 - accuracy: 0.7000 - val_loss: 1.1373 - val_accuracy: 0.5667\n",
            "Epoch 15/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0534 - accuracy: 0.6917 - val_loss: 1.1236 - val_accuracy: 0.5667\n",
            "Epoch 16/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0353 - accuracy: 0.7083 - val_loss: 1.1035 - val_accuracy: 0.5667\n",
            "Epoch 17/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0194 - accuracy: 0.7083 - val_loss: 1.0883 - val_accuracy: 0.5667\n",
            "Epoch 18/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0036 - accuracy: 0.7083 - val_loss: 1.0755 - val_accuracy: 0.5667\n",
            "Epoch 19/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9919 - accuracy: 0.7000 - val_loss: 1.0640 - val_accuracy: 0.5667\n",
            "Epoch 20/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9762 - accuracy: 0.7083 - val_loss: 1.0497 - val_accuracy: 0.5667\n",
            "Epoch 21/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9637 - accuracy: 0.7083 - val_loss: 1.0429 - val_accuracy: 0.5667\n",
            "Epoch 22/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9504 - accuracy: 0.7083 - val_loss: 1.0242 - val_accuracy: 0.5667\n",
            "Epoch 23/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9405 - accuracy: 0.7167 - val_loss: 1.0082 - val_accuracy: 0.6000\n",
            "Epoch 24/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9268 - accuracy: 0.7167 - val_loss: 1.0022 - val_accuracy: 0.6000\n",
            "Epoch 25/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9156 - accuracy: 0.7167 - val_loss: 0.9895 - val_accuracy: 0.6000\n",
            "Epoch 26/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9046 - accuracy: 0.7167 - val_loss: 0.9787 - val_accuracy: 0.6000\n",
            "Epoch 27/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8930 - accuracy: 0.7167 - val_loss: 0.9687 - val_accuracy: 0.6000\n",
            "Epoch 28/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8836 - accuracy: 0.7167 - val_loss: 0.9535 - val_accuracy: 0.6333\n",
            "Epoch 29/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8718 - accuracy: 0.7167 - val_loss: 0.9425 - val_accuracy: 0.6333\n",
            "Epoch 30/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8605 - accuracy: 0.7667 - val_loss: 0.9305 - val_accuracy: 0.6667\n",
            "Epoch 31/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8449 - accuracy: 0.7750 - val_loss: 0.9125 - val_accuracy: 0.6667\n",
            "Epoch 32/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8275 - accuracy: 0.7667 - val_loss: 0.8972 - val_accuracy: 0.6667\n",
            "Epoch 33/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8118 - accuracy: 0.7917 - val_loss: 0.8756 - val_accuracy: 0.7000\n",
            "Epoch 34/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7955 - accuracy: 0.8583 - val_loss: 0.8440 - val_accuracy: 0.7667\n",
            "Epoch 35/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7694 - accuracy: 0.8750 - val_loss: 0.8182 - val_accuracy: 0.7667\n",
            "Epoch 36/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.7287 - accuracy: 0.9333 - val_loss: 0.7516 - val_accuracy: 1.0000\n",
            "Epoch 37/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.9583 - val_loss: 0.7144 - val_accuracy: 0.9667\n",
            "Epoch 38/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6466 - accuracy: 0.9667 - val_loss: 0.7013 - val_accuracy: 0.9333\n",
            "Epoch 39/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6202 - accuracy: 0.9500 - val_loss: 0.6743 - val_accuracy: 0.9333\n",
            "Epoch 40/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.9583 - val_loss: 0.6542 - val_accuracy: 0.9333\n",
            "Epoch 41/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5773 - accuracy: 0.9583 - val_loss: 0.6316 - val_accuracy: 0.9333\n",
            "Epoch 42/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.9583 - val_loss: 0.6437 - val_accuracy: 0.9333\n",
            "Epoch 43/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5478 - accuracy: 0.9583 - val_loss: 0.5955 - val_accuracy: 0.9667\n",
            "Epoch 44/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5352 - accuracy: 0.9583 - val_loss: 0.5832 - val_accuracy: 0.9667\n",
            "Epoch 45/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5217 - accuracy: 0.9583 - val_loss: 0.5832 - val_accuracy: 0.9667\n",
            "Epoch 46/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5101 - accuracy: 0.9583 - val_loss: 0.5557 - val_accuracy: 0.9667\n",
            "Epoch 47/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4801 - accuracy: 0.9667 - val_loss: 0.5133 - val_accuracy: 0.9667\n",
            "Epoch 48/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4467 - accuracy: 0.9667 - val_loss: 0.4679 - val_accuracy: 0.9667\n",
            "Epoch 49/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.9750 - val_loss: 0.4616 - val_accuracy: 0.9667\n",
            "Epoch 50/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.9833 - val_loss: 0.3999 - val_accuracy: 0.9667\n",
            "Epoch 51/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3540 - accuracy: 0.9750 - val_loss: 0.3552 - val_accuracy: 0.9667\n",
            "Epoch 52/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3277 - accuracy: 0.9667 - val_loss: 0.3310 - val_accuracy: 0.9667\n",
            "Epoch 53/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3067 - accuracy: 0.9667 - val_loss: 0.3107 - val_accuracy: 0.9667\n",
            "Epoch 54/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2916 - accuracy: 0.9667 - val_loss: 0.2959 - val_accuracy: 0.9667\n",
            "Epoch 55/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2796 - accuracy: 0.9667 - val_loss: 0.2826 - val_accuracy: 0.9667\n",
            "Epoch 56/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2712 - accuracy: 0.9667 - val_loss: 0.2680 - val_accuracy: 0.9667\n",
            "Epoch 57/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2630 - accuracy: 0.9750 - val_loss: 0.2730 - val_accuracy: 0.9667\n",
            "Epoch 58/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2563 - accuracy: 0.9750 - val_loss: 0.2565 - val_accuracy: 0.9667\n",
            "Epoch 59/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2554 - accuracy: 0.9667 - val_loss: 0.2478 - val_accuracy: 0.9667\n",
            "Epoch 60/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2470 - accuracy: 0.9750 - val_loss: 0.2533 - val_accuracy: 0.9667\n",
            "Epoch 61/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2469 - accuracy: 0.9667 - val_loss: 0.2373 - val_accuracy: 0.9667\n",
            "Epoch 62/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2406 - accuracy: 0.9667 - val_loss: 0.2403 - val_accuracy: 0.9667\n",
            "Epoch 63/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2362 - accuracy: 0.9750 - val_loss: 0.2349 - val_accuracy: 0.9667\n",
            "Epoch 64/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2336 - accuracy: 0.9750 - val_loss: 0.2270 - val_accuracy: 0.9667\n",
            "Epoch 65/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9667 - val_loss: 0.2234 - val_accuracy: 0.9667\n",
            "Epoch 66/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9750 - val_loss: 0.2213 - val_accuracy: 0.9667\n",
            "Epoch 67/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2253 - accuracy: 0.9750 - val_loss: 0.2190 - val_accuracy: 0.9667\n",
            "Epoch 68/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2222 - accuracy: 0.9750 - val_loss: 0.2103 - val_accuracy: 0.9667\n",
            "Epoch 69/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2252 - accuracy: 0.9750 - val_loss: 0.2156 - val_accuracy: 0.9667\n",
            "Epoch 70/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2198 - accuracy: 0.9750 - val_loss: 0.2077 - val_accuracy: 0.9667\n",
            "Epoch 71/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.9750 - val_loss: 0.2041 - val_accuracy: 0.9667\n",
            "Epoch 72/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2136 - accuracy: 0.9750 - val_loss: 0.2034 - val_accuracy: 0.9667\n",
            "Epoch 73/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2124 - accuracy: 0.9750 - val_loss: 0.2015 - val_accuracy: 0.9667\n",
            "Epoch 74/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2095 - accuracy: 0.9750 - val_loss: 0.1965 - val_accuracy: 0.9667\n",
            "Epoch 75/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2081 - accuracy: 0.9750 - val_loss: 0.1949 - val_accuracy: 0.9667\n",
            "Epoch 76/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2073 - accuracy: 0.9750 - val_loss: 0.2008 - val_accuracy: 0.9667\n",
            "Epoch 77/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9750 - val_loss: 0.1919 - val_accuracy: 0.9667\n",
            "Epoch 78/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2032 - accuracy: 0.9750 - val_loss: 0.1973 - val_accuracy: 0.9667\n",
            "Epoch 79/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.9750 - val_loss: 0.1956 - val_accuracy: 0.9667\n",
            "Epoch 80/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9750 - val_loss: 0.1896 - val_accuracy: 0.9667\n",
            "Epoch 81/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9750 - val_loss: 0.1872 - val_accuracy: 0.9667\n",
            "Epoch 82/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1991 - accuracy: 0.9750 - val_loss: 0.1939 - val_accuracy: 0.9667\n",
            "Epoch 83/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1966 - accuracy: 0.9750 - val_loss: 0.1878 - val_accuracy: 0.9667\n",
            "Epoch 84/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1954 - accuracy: 0.9750 - val_loss: 0.1847 - val_accuracy: 0.9667\n",
            "Epoch 85/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1956 - accuracy: 0.9750 - val_loss: 0.1857 - val_accuracy: 0.9667\n",
            "Epoch 86/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1918 - accuracy: 0.9750 - val_loss: 0.1784 - val_accuracy: 0.9667\n",
            "Epoch 87/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1917 - accuracy: 0.9750 - val_loss: 0.1798 - val_accuracy: 0.9667\n",
            "Epoch 88/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9750 - val_loss: 0.1802 - val_accuracy: 0.9667\n",
            "Epoch 89/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1951 - accuracy: 0.9750 - val_loss: 0.1727 - val_accuracy: 0.9667\n",
            "Epoch 90/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1965 - accuracy: 0.9667 - val_loss: 0.2127 - val_accuracy: 0.9667\n",
            "Epoch 91/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9750 - val_loss: 0.1899 - val_accuracy: 0.9667\n",
            "Epoch 92/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1852 - accuracy: 0.9750 - val_loss: 0.1740 - val_accuracy: 0.9667\n",
            "Epoch 93/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9667 - val_loss: 0.1652 - val_accuracy: 0.9667\n",
            "Epoch 94/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.9750 - val_loss: 0.1825 - val_accuracy: 0.9667\n",
            "Epoch 95/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1838 - accuracy: 0.9750 - val_loss: 0.1759 - val_accuracy: 0.9667\n",
            "Epoch 96/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9667 - val_loss: 0.1824 - val_accuracy: 0.9667\n",
            "Epoch 97/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1821 - accuracy: 0.9750 - val_loss: 0.1747 - val_accuracy: 0.9667\n",
            "Epoch 98/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9750 - val_loss: 0.1641 - val_accuracy: 0.9667\n",
            "Epoch 99/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9750 - val_loss: 0.1726 - val_accuracy: 0.9667\n",
            "Epoch 100/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1802 - accuracy: 0.9750 - val_loss: 0.1696 - val_accuracy: 0.9667\n",
            "Epoch 101/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1781 - accuracy: 0.9750 - val_loss: 0.1685 - val_accuracy: 0.9667\n",
            "Epoch 102/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9750 - val_loss: 0.1713 - val_accuracy: 0.9667\n",
            "Epoch 103/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9750 - val_loss: 0.1668 - val_accuracy: 0.9667\n",
            "Epoch 104/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1747 - accuracy: 0.9750 - val_loss: 0.1622 - val_accuracy: 0.9667\n",
            "Epoch 105/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9833 - val_loss: 0.1639 - val_accuracy: 0.9667\n",
            "Epoch 106/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1816 - accuracy: 0.9833 - val_loss: 0.1956 - val_accuracy: 0.9667\n",
            "Epoch 107/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1794 - accuracy: 0.9750 - val_loss: 0.1700 - val_accuracy: 0.9667\n",
            "Epoch 108/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1741 - accuracy: 0.9750 - val_loss: 0.1620 - val_accuracy: 0.9667\n",
            "Epoch 109/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1717 - accuracy: 0.9750 - val_loss: 0.1626 - val_accuracy: 0.9667\n",
            "Epoch 110/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9750 - val_loss: 0.1660 - val_accuracy: 0.9667\n",
            "Epoch 111/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1711 - accuracy: 0.9750 - val_loss: 0.1599 - val_accuracy: 0.9667\n",
            "Epoch 112/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1706 - accuracy: 0.9750 - val_loss: 0.1660 - val_accuracy: 0.9667\n",
            "Epoch 113/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1688 - accuracy: 0.9750 - val_loss: 0.1579 - val_accuracy: 0.9667\n",
            "Epoch 114/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1711 - accuracy: 0.9750 - val_loss: 0.1497 - val_accuracy: 0.9667\n",
            "Epoch 115/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1682 - accuracy: 0.9750 - val_loss: 0.1607 - val_accuracy: 0.9667\n",
            "Epoch 116/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1671 - accuracy: 0.9750 - val_loss: 0.1602 - val_accuracy: 0.9667\n",
            "Epoch 117/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9750 - val_loss: 0.1584 - val_accuracy: 0.9667\n",
            "Epoch 118/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1677 - accuracy: 0.9833 - val_loss: 0.1438 - val_accuracy: 0.9667\n",
            "Epoch 119/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1689 - accuracy: 0.9750 - val_loss: 0.1558 - val_accuracy: 0.9667\n",
            "Epoch 120/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1646 - accuracy: 0.9750 - val_loss: 0.1584 - val_accuracy: 0.9667\n",
            "Epoch 121/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9750 - val_loss: 0.1561 - val_accuracy: 0.9667\n",
            "Epoch 122/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1622 - accuracy: 0.9750 - val_loss: 0.1600 - val_accuracy: 0.9667\n",
            "Epoch 123/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1628 - accuracy: 0.9750 - val_loss: 0.1565 - val_accuracy: 0.9667\n",
            "Epoch 124/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1628 - accuracy: 0.9833 - val_loss: 0.1596 - val_accuracy: 0.9667\n",
            "Epoch 125/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9833 - val_loss: 0.1391 - val_accuracy: 1.0000\n",
            "Epoch 126/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9833 - val_loss: 0.1429 - val_accuracy: 0.9667\n",
            "Epoch 127/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1608 - accuracy: 0.9833 - val_loss: 0.1536 - val_accuracy: 0.9667\n",
            "Epoch 128/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9833 - val_loss: 0.1486 - val_accuracy: 0.9667\n",
            "Epoch 129/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9833 - val_loss: 0.1518 - val_accuracy: 0.9667\n",
            "Epoch 130/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9833 - val_loss: 0.1496 - val_accuracy: 0.9667\n",
            "Epoch 131/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 0.9833 - val_loss: 0.1446 - val_accuracy: 0.9667\n",
            "Epoch 132/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9833 - val_loss: 0.1522 - val_accuracy: 0.9667\n",
            "Epoch 133/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9833 - val_loss: 0.1580 - val_accuracy: 0.9667\n",
            "Epoch 134/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9833 - val_loss: 0.1489 - val_accuracy: 0.9667\n",
            "Epoch 135/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9833 - val_loss: 0.1470 - val_accuracy: 0.9667\n",
            "Epoch 136/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1564 - accuracy: 0.9833 - val_loss: 0.1463 - val_accuracy: 0.9667\n",
            "Epoch 137/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9833 - val_loss: 0.1458 - val_accuracy: 0.9667\n",
            "Epoch 138/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1559 - accuracy: 0.9833 - val_loss: 0.1443 - val_accuracy: 0.9667\n",
            "Epoch 139/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9833 - val_loss: 0.1487 - val_accuracy: 0.9667\n",
            "Epoch 140/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1541 - accuracy: 0.9833 - val_loss: 0.1401 - val_accuracy: 0.9667\n",
            "Epoch 141/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9833 - val_loss: 0.1436 - val_accuracy: 0.9667\n",
            "Epoch 142/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9833 - val_loss: 0.1451 - val_accuracy: 0.9667\n",
            "Epoch 143/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9833 - val_loss: 0.1424 - val_accuracy: 0.9667\n",
            "Epoch 144/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9833 - val_loss: 0.1425 - val_accuracy: 0.9667\n",
            "Epoch 145/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9833 - val_loss: 0.1455 - val_accuracy: 0.9667\n",
            "Epoch 146/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9833 - val_loss: 0.1500 - val_accuracy: 0.9667\n",
            "Epoch 147/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1515 - accuracy: 0.9833 - val_loss: 0.1385 - val_accuracy: 0.9667\n",
            "Epoch 148/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9833 - val_loss: 0.1440 - val_accuracy: 0.9667\n",
            "Epoch 149/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9833 - val_loss: 0.1389 - val_accuracy: 0.9667\n",
            "Epoch 150/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9833 - val_loss: 0.1405 - val_accuracy: 0.9667\n",
            "Epoch 151/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1503 - accuracy: 0.9833 - val_loss: 0.1505 - val_accuracy: 0.9667\n",
            "Epoch 152/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9917 - val_loss: 0.1427 - val_accuracy: 0.9667\n",
            "Epoch 153/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.9833 - val_loss: 0.1461 - val_accuracy: 0.9667\n",
            "Epoch 154/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9833 - val_loss: 0.1442 - val_accuracy: 0.9667\n",
            "Epoch 155/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9750 - val_loss: 0.1238 - val_accuracy: 1.0000\n",
            "Epoch 156/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1580 - accuracy: 0.9750 - val_loss: 0.1401 - val_accuracy: 0.9667\n",
            "Epoch 157/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9833 - val_loss: 0.1437 - val_accuracy: 0.9667\n",
            "Epoch 158/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1469 - accuracy: 0.9833 - val_loss: 0.1467 - val_accuracy: 0.9667\n",
            "Epoch 159/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1472 - accuracy: 0.9833 - val_loss: 0.1453 - val_accuracy: 0.9667\n",
            "Epoch 160/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1479 - accuracy: 0.9833 - val_loss: 0.1423 - val_accuracy: 0.9667\n",
            "Epoch 161/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9833 - val_loss: 0.1440 - val_accuracy: 0.9667\n",
            "Epoch 162/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9833 - val_loss: 0.1392 - val_accuracy: 0.9667\n",
            "Epoch 163/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.9833 - val_loss: 0.1428 - val_accuracy: 0.9667\n",
            "Epoch 164/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9833 - val_loss: 0.1370 - val_accuracy: 0.9667\n",
            "Epoch 165/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9833 - val_loss: 0.1451 - val_accuracy: 0.9667\n",
            "Epoch 166/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9833 - val_loss: 0.1342 - val_accuracy: 0.9667\n",
            "Epoch 167/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9833 - val_loss: 0.1428 - val_accuracy: 0.9667\n",
            "Epoch 168/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1441 - accuracy: 0.9833 - val_loss: 0.1381 - val_accuracy: 0.9667\n",
            "Epoch 169/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9833 - val_loss: 0.1453 - val_accuracy: 0.9667\n",
            "Epoch 170/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9833 - val_loss: 0.1447 - val_accuracy: 0.9667\n",
            "Epoch 171/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1427 - accuracy: 0.9833 - val_loss: 0.1483 - val_accuracy: 0.9667\n",
            "Epoch 172/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9833 - val_loss: 0.1427 - val_accuracy: 0.9667\n",
            "Epoch 173/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1430 - accuracy: 0.9833 - val_loss: 0.1387 - val_accuracy: 0.9667\n",
            "Epoch 174/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9833 - val_loss: 0.1417 - val_accuracy: 0.9667\n",
            "Epoch 175/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1426 - accuracy: 0.9833 - val_loss: 0.1378 - val_accuracy: 0.9667\n",
            "Epoch 176/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9833 - val_loss: 0.1354 - val_accuracy: 0.9667\n",
            "Epoch 177/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1426 - accuracy: 0.9833 - val_loss: 0.1337 - val_accuracy: 0.9667\n",
            "Epoch 178/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1408 - accuracy: 0.9833 - val_loss: 0.1393 - val_accuracy: 0.9667\n",
            "Epoch 179/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9833 - val_loss: 0.1419 - val_accuracy: 0.9667\n",
            "Epoch 180/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9833 - val_loss: 0.1382 - val_accuracy: 0.9667\n",
            "Epoch 181/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9833 - val_loss: 0.1342 - val_accuracy: 0.9667\n",
            "Epoch 182/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1390 - accuracy: 0.9833 - val_loss: 0.1346 - val_accuracy: 0.9667\n",
            "Epoch 183/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9833 - val_loss: 0.1327 - val_accuracy: 0.9667\n",
            "Epoch 184/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 0.9833 - val_loss: 0.1370 - val_accuracy: 0.9667\n",
            "Epoch 185/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1388 - accuracy: 0.9833 - val_loss: 0.1371 - val_accuracy: 0.9667\n",
            "Epoch 186/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9833 - val_loss: 0.1373 - val_accuracy: 0.9667\n",
            "Epoch 187/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1380 - accuracy: 0.9833 - val_loss: 0.1296 - val_accuracy: 0.9667\n",
            "Epoch 188/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9833 - val_loss: 0.1304 - val_accuracy: 0.9667\n",
            "Epoch 189/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9833 - val_loss: 0.1331 - val_accuracy: 0.9667\n",
            "Epoch 190/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1372 - accuracy: 0.9833 - val_loss: 0.1380 - val_accuracy: 0.9667\n",
            "Epoch 191/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1368 - accuracy: 0.9833 - val_loss: 0.1390 - val_accuracy: 0.9667\n",
            "Epoch 192/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9833 - val_loss: 0.1325 - val_accuracy: 0.9667\n",
            "Epoch 193/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9833 - val_loss: 0.1351 - val_accuracy: 0.9667\n",
            "Epoch 194/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9833 - val_loss: 0.1359 - val_accuracy: 0.9667\n",
            "Epoch 195/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1376 - accuracy: 0.9833 - val_loss: 0.1299 - val_accuracy: 0.9667\n",
            "Epoch 196/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9833 - val_loss: 0.1330 - val_accuracy: 0.9667\n",
            "Epoch 197/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9833 - val_loss: 0.1303 - val_accuracy: 0.9667\n",
            "Epoch 198/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1376 - accuracy: 0.9833 - val_loss: 0.1320 - val_accuracy: 0.9667\n",
            "Epoch 199/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9833 - val_loss: 0.1415 - val_accuracy: 0.9667\n",
            "Epoch 200/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1345 - accuracy: 0.9917 - val_loss: 0.1307 - val_accuracy: 0.9667\n",
            "Epoch 201/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9833 - val_loss: 0.1261 - val_accuracy: 0.9667\n",
            "Epoch 202/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1348 - accuracy: 0.9833 - val_loss: 0.1308 - val_accuracy: 0.9667\n",
            "Epoch 203/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9833 - val_loss: 0.1333 - val_accuracy: 0.9667\n",
            "Epoch 204/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9833 - val_loss: 0.1284 - val_accuracy: 0.9667\n",
            "Epoch 205/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9833 - val_loss: 0.1313 - val_accuracy: 0.9667\n",
            "Epoch 206/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1336 - accuracy: 0.9917 - val_loss: 0.1374 - val_accuracy: 0.9667\n",
            "Epoch 207/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9833 - val_loss: 0.1398 - val_accuracy: 0.9667\n",
            "Epoch 208/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1340 - accuracy: 0.9833 - val_loss: 0.1333 - val_accuracy: 0.9667\n",
            "Epoch 209/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9833 - val_loss: 0.1277 - val_accuracy: 0.9667\n",
            "Epoch 210/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9833 - val_loss: 0.1312 - val_accuracy: 0.9667\n",
            "Epoch 211/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9833 - val_loss: 0.1278 - val_accuracy: 0.9667\n",
            "Epoch 212/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9833 - val_loss: 0.1353 - val_accuracy: 0.9667\n",
            "Epoch 213/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9833 - val_loss: 0.1318 - val_accuracy: 0.9667\n",
            "Epoch 214/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9833 - val_loss: 0.1302 - val_accuracy: 0.9667\n",
            "Epoch 215/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1315 - accuracy: 0.9833 - val_loss: 0.1382 - val_accuracy: 0.9667\n",
            "Epoch 216/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9833 - val_loss: 0.1334 - val_accuracy: 0.9667\n",
            "Epoch 217/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1316 - accuracy: 0.9833 - val_loss: 0.1307 - val_accuracy: 0.9667\n",
            "Epoch 218/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9750 - val_loss: 0.1154 - val_accuracy: 0.9667\n",
            "Epoch 219/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9833 - val_loss: 0.1299 - val_accuracy: 0.9667\n",
            "Epoch 220/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9833 - val_loss: 0.1352 - val_accuracy: 0.9667\n",
            "Epoch 221/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1317 - accuracy: 0.9833 - val_loss: 0.1258 - val_accuracy: 0.9667\n",
            "Epoch 222/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1305 - accuracy: 0.9833 - val_loss: 0.1264 - val_accuracy: 0.9667\n",
            "Epoch 223/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9833 - val_loss: 0.1275 - val_accuracy: 0.9667\n",
            "Epoch 224/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.9833 - val_loss: 0.1326 - val_accuracy: 0.9667\n",
            "Epoch 225/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9833 - val_loss: 0.1244 - val_accuracy: 0.9667\n",
            "Epoch 226/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9833 - val_loss: 0.1221 - val_accuracy: 0.9667\n",
            "Epoch 227/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1384 - accuracy: 0.9833 - val_loss: 0.1695 - val_accuracy: 0.9667\n",
            "Epoch 228/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9750 - val_loss: 0.1389 - val_accuracy: 0.9667\n",
            "Epoch 229/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1498 - accuracy: 0.9667 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
            "Epoch 230/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9750 - val_loss: 0.1256 - val_accuracy: 0.9667\n",
            "Epoch 231/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1274 - accuracy: 0.9833 - val_loss: 0.1381 - val_accuracy: 0.9667\n",
            "Epoch 232/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9833 - val_loss: 0.1403 - val_accuracy: 0.9667\n",
            "Epoch 233/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9833 - val_loss: 0.1252 - val_accuracy: 0.9667\n",
            "Epoch 234/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1299 - accuracy: 0.9833 - val_loss: 0.1245 - val_accuracy: 0.9667\n",
            "Epoch 235/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9833 - val_loss: 0.1291 - val_accuracy: 0.9667\n",
            "Epoch 236/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9833 - val_loss: 0.1285 - val_accuracy: 0.9667\n",
            "Epoch 237/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1285 - accuracy: 0.9833 - val_loss: 0.1259 - val_accuracy: 0.9667\n",
            "Epoch 238/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9833 - val_loss: 0.1259 - val_accuracy: 0.9667\n",
            "Epoch 239/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9833 - val_loss: 0.1241 - val_accuracy: 0.9667\n",
            "Epoch 240/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1273 - accuracy: 0.9833 - val_loss: 0.1257 - val_accuracy: 0.9667\n",
            "Epoch 241/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9833 - val_loss: 0.1208 - val_accuracy: 0.9667\n",
            "Epoch 242/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9833 - val_loss: 0.1243 - val_accuracy: 0.9667\n",
            "Epoch 243/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9833 - val_loss: 0.1212 - val_accuracy: 0.9667\n",
            "Epoch 244/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9833 - val_loss: 0.1226 - val_accuracy: 0.9667\n",
            "Epoch 245/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1266 - accuracy: 0.9833 - val_loss: 0.1220 - val_accuracy: 0.9667\n",
            "Epoch 246/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9833 - val_loss: 0.1235 - val_accuracy: 0.9667\n",
            "Epoch 247/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1263 - accuracy: 0.9833 - val_loss: 0.1269 - val_accuracy: 0.9667\n",
            "Epoch 248/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1265 - accuracy: 0.9833 - val_loss: 0.1278 - val_accuracy: 0.9667\n",
            "Epoch 249/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9833 - val_loss: 0.1221 - val_accuracy: 0.9667\n",
            "Epoch 250/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9833 - val_loss: 0.1222 - val_accuracy: 0.9667\n",
            "Epoch 251/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9833 - val_loss: 0.1280 - val_accuracy: 0.9667\n",
            "Epoch 252/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9833 - val_loss: 0.1194 - val_accuracy: 0.9667\n",
            "Epoch 253/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9833 - val_loss: 0.1196 - val_accuracy: 0.9667\n",
            "Epoch 254/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1252 - accuracy: 0.9833 - val_loss: 0.1246 - val_accuracy: 0.9667\n",
            "Epoch 255/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1273 - accuracy: 0.9833 - val_loss: 0.1202 - val_accuracy: 0.9667\n",
            "Epoch 256/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9833 - val_loss: 0.1286 - val_accuracy: 0.9667\n",
            "Epoch 257/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9917 - val_loss: 0.1291 - val_accuracy: 0.9667\n",
            "Epoch 258/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1242 - accuracy: 0.9833 - val_loss: 0.1222 - val_accuracy: 0.9667\n",
            "Epoch 259/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9833 - val_loss: 0.1176 - val_accuracy: 0.9667\n",
            "Epoch 260/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9833 - val_loss: 0.1296 - val_accuracy: 0.9667\n",
            "Epoch 261/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9833 - val_loss: 0.1298 - val_accuracy: 0.9667\n",
            "Epoch 262/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.9833 - val_loss: 0.1233 - val_accuracy: 0.9667\n",
            "Epoch 263/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9833 - val_loss: 0.1238 - val_accuracy: 0.9667\n",
            "Epoch 264/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9833 - val_loss: 0.1276 - val_accuracy: 0.9667\n",
            "Epoch 265/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9833 - val_loss: 0.1239 - val_accuracy: 0.9667\n",
            "Epoch 266/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9833 - val_loss: 0.1290 - val_accuracy: 0.9667\n",
            "Epoch 267/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9833 - val_loss: 0.1230 - val_accuracy: 0.9667\n",
            "Epoch 268/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9833 - val_loss: 0.1170 - val_accuracy: 0.9667\n",
            "Epoch 269/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1235 - accuracy: 0.9833 - val_loss: 0.1231 - val_accuracy: 0.9667\n",
            "Epoch 270/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1229 - accuracy: 0.9833 - val_loss: 0.1226 - val_accuracy: 0.9667\n",
            "Epoch 271/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9833 - val_loss: 0.1247 - val_accuracy: 0.9667\n",
            "Epoch 272/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1222 - accuracy: 0.9833 - val_loss: 0.1235 - val_accuracy: 0.9667\n",
            "Epoch 273/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1234 - accuracy: 0.9833 - val_loss: 0.1259 - val_accuracy: 0.9667\n",
            "Epoch 274/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.9833 - val_loss: 0.1170 - val_accuracy: 0.9667\n",
            "Epoch 275/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1228 - accuracy: 0.9833 - val_loss: 0.1223 - val_accuracy: 0.9667\n",
            "Epoch 276/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1214 - accuracy: 0.9833 - val_loss: 0.1311 - val_accuracy: 0.9667\n",
            "Epoch 277/300\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9833 - val_loss: 0.1309 - val_accuracy: 0.9667\n",
            "Epoch 278/300\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.1246 - accuracy: 0.9833 - val_loss: 0.1191 - val_accuracy: 0.9667\n",
            "Epoch 279/300\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1217 - accuracy: 0.9833 - val_loss: 0.1197 - val_accuracy: 0.9667\n",
            "Epoch 280/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.9833 - val_loss: 0.1137 - val_accuracy: 0.9667\n",
            "Epoch 281/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1213 - accuracy: 0.9833 - val_loss: 0.1217 - val_accuracy: 0.9667\n",
            "Epoch 282/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.9833 - val_loss: 0.1242 - val_accuracy: 0.9667\n",
            "Epoch 283/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9833 - val_loss: 0.1249 - val_accuracy: 0.9667\n",
            "Epoch 284/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.9833 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
            "Epoch 285/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9833 - val_loss: 0.1643 - val_accuracy: 0.9667\n",
            "Epoch 286/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9750 - val_loss: 0.1365 - val_accuracy: 0.9667\n",
            "Epoch 287/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1236 - accuracy: 0.9833 - val_loss: 0.1244 - val_accuracy: 0.9667\n",
            "Epoch 288/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9833 - val_loss: 0.1269 - val_accuracy: 0.9667\n",
            "Epoch 289/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9833 - val_loss: 0.1234 - val_accuracy: 0.9667\n",
            "Epoch 290/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9833 - val_loss: 0.1247 - val_accuracy: 0.9667\n",
            "Epoch 291/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9833 - val_loss: 0.1228 - val_accuracy: 0.9667\n",
            "Epoch 292/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.9833 - val_loss: 0.1114 - val_accuracy: 0.9667\n",
            "Epoch 293/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1209 - accuracy: 0.9833 - val_loss: 0.1178 - val_accuracy: 0.9667\n",
            "Epoch 294/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9833 - val_loss: 0.1225 - val_accuracy: 0.9667\n",
            "Epoch 295/300\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9833 - val_loss: 0.1211 - val_accuracy: 0.9667\n",
            "Epoch 296/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9833 - val_loss: 0.1204 - val_accuracy: 0.9667\n",
            "Epoch 297/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1196 - accuracy: 0.9833 - val_loss: 0.1164 - val_accuracy: 0.9667\n",
            "Epoch 298/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9833 - val_loss: 0.1180 - val_accuracy: 0.9667\n",
            "Epoch 299/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1204 - accuracy: 0.9833 - val_loss: 0.1192 - val_accuracy: 0.9667\n",
            "Epoch 300/300\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1190 - accuracy: 0.9833 - val_loss: 0.1186 - val_accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pyplot.title('Loss / Mean Squared Error')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# Your input to confusion_matrix must be an array of int not one hot encodings.\n",
        "y_pred = model.predict(X_testscaled)\n",
        "cf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(cf_matrix)\n",
        "\n",
        "model.evaluate(X_testscaled,y_test)\n",
        "\n",
        "\n",
        "print('Output activation by default: {}'.format(net.out_activation_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "jMSjEeFkc7qr",
        "outputId": "4087c2c7-35b8-4ae6-da4e-08f437d8b7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 301ms/step\n",
            "[[11  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0  6]]\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.1186 - accuracy: 0.9667\n",
            "Output activation by default: softmax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load irisAll.csv files as a Pandas DataFrame\n",
        "# https://www.kaggle.com/arshid/iris-flower-dataset\n",
        "data = pd.read_csv(\"Iris.csv\")"
      ],
      "metadata": {
        "id": "EChNqZ3IdICF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = data.dtypes[data.dtypes == \"object\"].index\n",
        "print(categorical)\n",
        "\n",
        "data[categorical].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "6oryaRvfdMug",
        "outputId": "807a5d61-9aa5-47a6-816b-dde1abfc7ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Species'], dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Species\n",
              "count           150\n",
              "unique            3\n",
              "top     Iris-setosa\n",
              "freq             50"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38dc3684-2e5e-4649-9596-29deaa8dc020\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38dc3684-2e5e-4649-9596-29deaa8dc020')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38dc3684-2e5e-4649-9596-29deaa8dc020 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38dc3684-2e5e-4649-9596-29deaa8dc020');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiFgwJWZrSji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}